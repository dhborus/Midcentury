## **Chapter Five:**

## **American Inclusion and Its Discontents**


Few, if any, photographic exhibitions have achieved the popularity of the Museum of Modern Art’s 1955 venture, “Family of Man.” Curated by famed photographer Edward Steichen, the undertaking consisted of  503 distinct images taken in sixty-eight countries by 273 different photographers. After its New York debut, the show spent the next eight years touring the world, stopping in thirty-seven nations before being permanently installed at Common Market headquarters in Steichen’s native Luxembourg. In toto, nine million people saw some version of the exhibit. A book version, which is still in print, has sold four million copies. As the title suggests, Steichen hoped the exhibit would convey the underlying and deep connections among the peoples of the world. To that end, he grouped the images by activities and conditions common to all—birth, work, play, courtship, marriage, death—rather than by country of origin. The panels of text that accompanied the image, written by Steichen’s brother-in-law, Pulitzer-Prize winning poet Carl Sandburg, announced that there “is only one man in the world and his name is All Men. There is only one woman in the world and her name is All Women.” Humanity was, Sandburg maintained, “one big family hugging close to the ball of Earth for its life and being.” Capping the exhibit was a photograph of the mushroom cloud, a vivid reminder that nuclear weapons constituted a common threat to the whole of humanity. Echoing the endorsement of underlying commonality, photographer Barbara Morgan praised the exhibition for its ability to have American viewers look at a polygamist family and feel an intrinsic kinship.[^1]

Not everyone applauded the exhibition or approved of its ideological position. Writing in 1958, famed photographer Walker Evans criticized its treacly sentimentality (“bogus heartfeeling”), which resulted from its false universalism (“human familyhood”). Evans's condemnation echoed one that the French critic Roland Barthes leveled a year earlier but which did not appear in English until 1970. Barthes criticized the “Family of Man” as a mythology, an ideological maneuver that aspired to naturalize history, robbing it of its particularity and its true diversity by postulating a magic transformation in which a surface pluralism gives way to a common mold in which “man is born, works, laughs and dies everywhere in the same way.” Citing the fate of Emmet Till, the black Chicago teenager brutally murdered in Mississippi for ostensibly leering at a white woman, Barthes contended that fellow feeling rarely, if ever, canceled awareness of  group differences.[^2]

Steichen's exhibition was just one of many cultural artifacts that emphasized social inclusion and solidarity. Observers rightly noted the growing talk of acceptance of those previously excluded and marginalized. Films about World War II often mirrored the "Family of Man" in their depiction of diverse military units whose members learn that acceptance of one another was the American way. It was that sentiment, so contrary to long-standing complaints about the United States becoming a polyglot boardinghouse, that led the editors of *Fortune* magazine to assert somewhat blithely that “The presence . . . of a bewildering number of races and national origins, creeds and shibboleths, economic interests and explosive ideas is to him [the American] no problem at all. On the contrary it is a great asset.”[^3]

Inclusiveness as value and fact constituted a prime characteristic of academic discourse, as well. Numerous political scientists rejected theories of political power as flowing from a ruling elite for one that stressed its dispersal among competing, often newly organized and empowered interest groups. For their part, sociologists asserted that racial segregation ran afoul of the democratic creed at the heart of national identity and that failure to live by that creed led to social disturbance. Historians sought a key to national genius in immigration and assimilation. [^4] In highlighting American diversity and inclusion, American social science implicitly contrasted the variety and acceptance supposedly inherent in American social life with the gray and somber uniformity of totalitarian societies and bolstered Cold Ware-era appeals to decolonizing world.[^5]

Those who discerned an inclusionary spirit and wide dispersal of power conceded the phenomena were often more aspirational than descriptive. Few denied that exclusion, hierarchy, and domination were common occurrences in postwar American life, but many observers recognized enough signs of change to anticipate that inclusion and broad participation were becoming more than empty ideals. Although the 1960s have a reputation as the high tide of postwar reform, the immediate postwar period witnessed, among other noteworthy developments, an increase in college matriculation from families that had never had a member attend, the greater availability of luxury goods, a growing acceptance of Jews and Catholics as legitimate and co-equal participants in American society, the endorsement of full citizenship rights for people of color by the Democratic Party in the North and the Supreme Court (two institutions that had vehemently opposed that goal in the nineteenth century), a dawning perception that domesticity and passivity constituted unproductive constraints on women, and even a stirring of homosexual rights organizations in coastal cities and, in such fields as music and art, tolerance of gays and lesbians. The assumption that social inclusivity was increasing provided the foundation for Howard Becker’s classic study _Outsiders_ (1963), which argued that deviants were not inherently marginal or ill-suited for social life but were men and women who chose to reject social rules. Their deviance was a matter of attitude rather than birth or necessity.[^6]

Those who could not easily efface their differences, however, found the actuality of inclusion somewhat less glorious than the promise. Racial minorities and women both faced a dedicated opposition convinced that their biological and psychological makeup, which supposedly made them incapable of complete and productive participation. An increasing number of activists resisted marginalization, forcefully iterating that their common humanity qualified them for social, cultural, and political inclusion. The excruciatingly slow pace of change, however, led a small but incisive minority of the excluded to reconsider the terms of inclusion. Rather than claim racial minorities and women were essentially no different from whites and men, they insisted that they possessed unique qualities that needed to be recognized and cultivated. Inclusions of such qualities, they averred, were necessary to any definition of the universally "human." Such a goal was, given American practices, extraordinarily difficult to realize. American music was no exception, riven as it was by cultural appropriations and racist business practices, but at rare moments as when Bill Evans sat down with Miles Davis, one might envision legitimate boundary crossings.

[^1]:	Edward Steichen, _The Family of Man_ (New York: Published for the Museum of Modern Art by the Maco Magazine Corp., 1955); Barbara Morgan, “The Theme Show: A Contemporary Exhibition Technique,” _Aperture_ 3(1955).

[^2]:	Walker Evans, “Robert Frank,” US Camera 1958 (New York: US Camera Publishing Corporation, 1957), 90; Roland Barthes, “The Great Family of Man,” in _Mythologies_ (New York: Hill & Wang, [1957] 1970), 100-3.

[^3]:	Russell Davenport, _U. S. A.: The Permanent Revolution_ (New York: Prentice-Hall, 1951), 8-9.

[^4]:	Robert A. Dahl, _A Preface to Democratic Theory_ (Chicago: University of Chicago Press, 1956); Gunnar Myrdal, _et al._, _An American Dilemma: The Negro Problem and Modern Democracy_ (New York, London: Harper & Brothers, 1944); Oscar Handlin, _The Uprooted: The Epic Story of the Great Migrations That Made the American People._ (Boston: Little, Brown, 1951).

[^5]:	See, for instance, Mary L. Dudziak, _Cold War Civil Rights: Race and the Image of American Democracy_ (Princeton, N.J.: Princeton University Press, 2000). For the uses of expressive culture as proof of American freedom, see, among others, Penny M. Von Eschen, _Satchmo Blows Up the World: Jazz Ambassadors Play the Cold War_ (Cambridge: Harvard University Press, 2004) and Serge Guilbaut, _How New York Stole the Idea of Modern Art: Abstract Expressionism, Freedom, and the Cold War_ (Chicago: University of Chicago Press, 1983).

[^6]:	Howard S. Becker, _Outsiders: Studies in the Sociology of Deviance._ (New York: Free Press, 1963).




### The Ambiguities of Belonging





After the war, observers of industrial life frequently concluded that the working class had sufficiently matured to be a responsible partner in securing economic growth. The judgment was particularly surprising given the strike wave of 1945-1946 in which nearly five million workers left the job. The wave struck such core industrial sectors as rails, steel, coal, oil, meat packing and automobiles and even resulted in general strikes in Lancaster, Pennsylvania, Rochester, New York, and Oakland, California. The wave in turn prompted condemnations from middle-class consumers and editorials boards across the country for allegedly interfering with prosperous postwar reconversion. Angered voters returned a Republican Congress, which passed the Taft-Hartley Act (1947) over President Truman's veto.  The Act limited the power of organized labor by outlawing the closed shop, allowing states to pass so-called right-to-work laws that prohibited contracts that made union membership a condition of employment, and requiring unions to give eighty days' notice of intention to strike. The strife of the late 1940s, however, proved to be the last gasp of the previous era rather than a foreshadowing of continued postwar conflict.

Although vestiges of the heated rhetoric of the 1930s remained in babor organizations' condemnation of Taft-Hartley as a “slave labor law” and conservative Arizona senator Barry Goldwater's claim that Auto Workers President Walter Reuther constituted a greater threat to freedom than Sputnik, many commentators were more impressed with how labor and management accepted, albeit grudgingly, the validity of the other's interests of the other side. The two sides, the argument went, had come to a mature understanding that their success depended upon that of their opposite. Large profits and high wages were interdependent. By 1951, _Fortune Magazine_  could praise unions for putting aside class-based thinking that made business the "ENEMY" and instead viewing management as the "opposing team in a rough and competitive game." Unions had become, the editors assured readers, the “tool for gaining and keeping as an individual the status and security of a full citizen in a capitalist society,” and making "the worker to an amazing degree a middle-class member of a middle-class society.” _Fortune_ did concede that unions were still capable of “group greed,” at times ignoring the public good such as when they insisted that members whose jobs were technologically obsolete continue to be paid. Labor spokespeople also applauded the end of class enmity. Congress of Industrial Organization President Phillip Murray told the organization's 1948 convention that “the interests of farmers, factory hands, business and professional people, and white-collar toilers prove to be the same.”[^7]

[^7]:	Davenport, _U. S. A.: The Permanent Revolution_, 91; Harvey Swados, _On the Line_ (Urbana: University of Illinois Press, 1990), xv-xvi.

Both *Fortune* and Murray pinned their argument about the concert of interests on the unprecedented expansion of consumption. Sustained profits and rising wages in the core sectors enabled an increasing number of workers to purchase discretionary items rather necessities. Many working-class families bought for the first time such goods as nylons, automobiles, vacations, and college educations, leading some commentators to talk about the growing democratization of social life. By the end of the period, air conditioners, washing machines, frozen foods, and television sets were less luxuries than part of the socially required basket of goods. Making the national spending spree possible was the liberal dispensation of credit. “Buy now, pay later” became a basic phrase of the American language. The Chamber of Commerce took the new consumerism as demonstration that the United States had developed a new, democratic form of capitalism. “U.S. capitalism is _popular_ capitalism, not only in the sense that it has popular support, but in the deeper sense that the people as a whole participate in it and use it,” crowed _Fortune_.[^8]

[^8]:	Davenport, _U. S. A.: The Permanent Revolution_, 67-68.

As postwar unions ceded control of the shop floor in exchange for long-term contracts that allowed members to maintain the new standard of living, such scholars as Princeton labor economist Richard A. Lester argued that the rank-and-file tended to identity as consumers rather than workers. Taking their cues from the attitudinal change of their “constituents,” labor organizations became experts in bargaining, eager to further the narrowing of “differences between manual and white-collar workers.” Lester argued that this “maturity” accounted for the greater middle-class acceptance of unions, which in turned encouraged workers to favor accumulation and individual advancement over solidarity.[^9] Interestingly, there was a companion literature that argued business had toned down its implacable opposition to organized labor and jettisoned the unfettered pursuit of profit. According to management expert Peter Drucker, the corporation had also matured, envisioning its function as serving customers rather than concentrating solely on augmenting gains. Service capitalism was especially respectful of workers, perhaps because those who worked with things were in the process of being replaced by those who labored in knowledge. Drucker's critics have regarded his work as unduly optimistic, especially in the light the labor policy of General Electric’s Lemuel Boulware which was designed to undermine the authority of labor union leadership with take-it-or-leave stance at every negotiation.[^10]

[^9]:	Richard Lester, _As Unions Mature: An Analysis of the Evolution of American Unions_ (Princeton: Princeton University Press, 1958), 111-2, 132.

[^10]:	Peter F. Drucker, _The New Society; the Anatomy of the Industrial Order._ (London: Heinemann, 1951); _idem_, _The Practice of Management._ (New York: Harper, 1954). Kim Phillips-Fein explores the hold of Boulwarism in  *Invisible Hands: The Businessmen’s Crusade Against the New Deal* (W. W. Norton & Company, 2010), especially pages 97-105.

Political scientists also assumed class was a less salient factor in political behavior. Robert Dahl's pluralist theory of "polyarchal democracy,'' an explicit challenge to both class analysis and C. Wright Mills's notion of a ruling elite, envisioned American politics as a clash between various interest groups, rooted in descent, occupation, religion, and race. Dahl argued that no one of these was dominant in every instance. In fact, successful politicians put together coalitions of groups that do not necessarily align with one another on more than one issue. This was true both locally, as he contended in his empirical study of New Haven, and nationally.[^11] Dahl’s pluralism, with its multiple focal points of power, would at first inspection seem far from a “centered” discourse. On the surface, Dahl wrote, the system “has so little order and so much chaos,” which nonetheless provided “a high probability that any active and legitimate group will make itself heard effectively at some stage in the process of decision.” What was centered in Dahl’s conception of politics was the dependence of polyarchal democracy on a social consensus. Dahl posited that the system of constant contentions and compromises flourished because interest groups generally agreed on the legitimacy of other actors and on the permissible goals and means. Beneath the political system that tended toward “reinforcing agreement, encouraging moderation, and maintaining social peace in a restless and immoderate people operating in a gigantic, powerful, diversified, and incredibly complex society” was nonetheless a social life that had effective mechanisms of inclusion rooted in assent and concurrence.[^12]

[^11]:	Dahl, _Who Governs? Democracy and Power in an American City_ (New Haven: Yale University Press, 1961).

[^12]:	Dahl, _Preface_, 132, 150-1.

Workers in core sectors may have consumed good that had previously been unavailble to much labor, but maintaing the standard of living had unintended consequences. As social commentator David Riesman pointed out in his introduction to Eli Chinoy’s classic study of Lansing, Michigan autoworkers, Chinoy’s autoworkers were “victimized by the growing prestige of consumer goods,” convinced that the “care and feeding of products” was the “mainstay of life.”[^13] On reflection, Chinoy found, many workers saw consumption as a trap that prevented advancement. Chinoy cited one worker who regarded his downfall as the purchase of a car because it chained him to the factory to make payments. Still another believed he might have been somebody if only he had put his mind to it and “not worshipped the almighty dollar.” Chinoy concluded that autoworkers believed they had to make a choice between occupational advancement and acquisition of material possessions. When accumulation of goods proved emotionally unsatisfying, influenced by the mythology of opportunity, they blamed themselves rather than existing economic arrangements for their failure to advance. Faced with their own lack of mobility, autoworkers put special emphasis on that of their children.[^14]

[^13]:	David Riesman, “Introduction,” Ely Chinoy, _Automobile Workers and the American Dream_ (Urbana: University of Illinois Press, 1992), xx-xxii.

[^14]:	Chinoy, _Automobile Workers and the American Dream_, especially 122-6. Confirming findings from Milpitas, California can be found in Bennett M. Berger, _Working-Class Suburb: A Study of Auto Workers in Suburbia._ (Berkeley: University of California Press, 1960).

Opting for enhanced consumption, Chinoy maintained, was compensation for the drudgery and alienation of the assembly line. That argument resonated with Harvey Swados’s. Swados was born in Buffalo, New York in 1920. As a young man, he affiliated with the anti-Stalinist Workers’ Party. During the war, he toiled in factories before joining the Navy. Following the Armistice, he worked in marketing and public relations while writing fiction. Returning from the south of France in 1956, he opted for factory work at the Ford plant in Mawah, New Jersey to support his family rather than return to marketing. Given that his intellectual friends offered him condolences when he told them of his decision, Swados doubted that workers had been fully incorporated into the American mainstream or that the classes were converging in any meaningful way. They had, as he maintained in his trenchant essay “The Myth of the Happy Worker,” considerably different life chances, assets, and beliefs. The vaunted enhanced consumption, even among relatively high paid auto workers, depended upon working overtime, second jobs and other family labor, and taking on larger debt loads.[^15]

[^15]:	Harvey Swados, “The Myth of the Happy Worker,” in Swados, _On the Line_, 235-57.

Rejecting the congratulatory huzzahs about workers becoming middle class, Swados saw alienation as constituting an unbridgeable gulf, an experience that managers and bosses did not have or fully grasp. Swados accepted that workers desired consumer goods, but he insisted that the meaning that they ascribed to accumulation and the function it served in their lives was distinct from middle-class norms. Workers’ lives were fraught with insecurities not just about their status but also their work. Anointing the standard of living the unifying element of American life, Swados contended, obscured the crucial difference in achieving it through physical exertion in noisy and dangerous places rather on one's bottom in clean and safe locales. Swados insisted that “there is one thing that the worker doesn’t do like the middle class: he works like a worker,” and his attitude toward his work “is generally compounded of hatred, shame and resignation” (237). In his gripping collection of short stories about auto work and workers, _On the Line_, he dramatized the frustrations and disappointments in working on the line as well as the dangers and dehumanizing regimentation. The stories highlight the ambiguity with which workers regard the automobile --- an appealing consumer item that holds the possibilities of freedom of movement while also chaining the worker in debt or even ending life on the factory floor or on the nation’s highways.

White ethnics were an especially celebrated success story of postwar inclusion. Once considered so unassimilable that Anglo-Americans excluded them from "white" status, descendants of immigrants from Southern and Eastern Europe had by the 1950s become full-fledged Americans in the eyes of most native born. The complexions, customs, religion, and language that had supposedly disqualified them no longer mattered, especially as the second and third generations eagerly adopted mainstream ways. Surveys indicated larger numbers of Americans spoke English as their first tongue than before the war. As historian Thomas Archdeacon has noted, the economic gap separating white ethnic nationalities from natives and from each other narrowed substantially. Many in the second and third generations left enclaves and moved to more mixed neighborhoods. The result was a change in identity. As John Higham noted, those who hailed from Poland went from being Poles in America to Polish-Americans to American of Polish descent or derivation.[^16] By the same token, nativism declined precipitously. Even the link between radicalism and national origin had loosened. Anti-Communists continued to decry Communism as an un-American creed advanced by foreigners, yet the author of the McCarran-Walter Act of 1952, which prohibited the entrance of “subversives,” was careful to indicate that immigrants as a whole were not a problem and many had been essential to American greatness. For his part, Joseph McCarthy was not particularly anti-Semitic.[^17] This integration was one reason by the 1950s the term “race,” which once was applied to Slavs or Hebrews, was reserved for nonwhites.[^18]


[^16]:	Thomas J. Archdeacon, _Becoming American: An Ethnic History_ (New York: Free Press; Collier Macmillan, 1983); Thomas J. Archdeacon, “Melting Pot or Cultural Pluralism? Changing Views on American Ethnicity,” _Revue Europeenne des Migrations Internationales_ 6, no. 1 (1990), 14-16; and John Higham, “Cultural Responses to Immigration,” in _Diversity and Its Discontents: Cultural Conflict and Common Ground in Contemporary American Society_, ed. Neil Smelser and Jeffrey Alexander (Princeton: Princeton University Press, 1999), 53. See also E. Allen Richardson, _Strangers in This Land: Pluralism and the Response to Diversity in the United States_ (Jefferson, NC: McFarland, 2010), 7-167.

[^17]:	Richard Polenberg, _One Nation Divisible: Class, Race, and Ethnicity in the United States Since 1938_ (New York: Penguin Books, 1991), 122-26.

[^18]:	Matthew Frye Jacobson, _Whiteness of a Different Color: European Immigrants and the Alchemy of Race_ (Cambridge, Mass.: Harvard University Press, 1998); Nell Irvin. Painter, _The History of White People_ (New York: W.W. Norton, 2010).

Postwar assimilation contrasted with the cultural pluralism that Horace Kallen had endorsed at the beginning of the century. Kallen had understood immigrant groups as pieces in a mosaic with each group more or less retaining its essential character. Americans would share a public culture but large swathes would be lived in ethnic enclaves. Writing in 1964, sociologist Milton Gordon contended that the nation had replaced the ancestral group as the source of the customs, values, and behaviors with which most white Americans identified. Marriage and place of residence ceased to be predominantly ethnic, although people tended to marry and live among those who shared their religious and class identifications -- choices that hastened the loss of ethnic identification.[^19]

[^19]:	Milton Gordon, _Assimilation in American Life: The Role of Race, Religion, and National Origins._ (New York: Oxford University Press, 1964).

Gordon built on Will Herberg’s concept of the triple melting pot which Herberg introduced in his _Protestant, Catholic, Jew_ (1955), which traced how religion became the vehicle of assimilation. Because it offered a shared sense of language, custom, and religion, immigrants had clung to national origin for security in the face of native hostility. Such identities served well those denied acceptance or who intended to return to their native lands. Although the first generation were relatively isolated  from American norms, Heberg argued that the second  attempted to rid itself of “immigrant foreignness” in order to benefit from “the extraordinary mobility of American society.” Identifying with their families’ communities held them back but efforts to throw off those connections left them without real foundations. Adding to the alienation was a movement away from the ethnic-infused religion of their parents. Herberg’s third generation accepted that being an American meant jettisoning the particularities of their descent groups but viewed religion as providing foundational identity compatible with Americanness.[^20]

[^20]:	Will Herberg, _Protestant, Catholic, Jew: An Essay in American Religious Sociology._ (Garden City, N.Y.: Anchor, 1955), 18-32.

In Herberg's rendition, the three faith groups actually eased entrance into the center of American life. They accomplished this feat because they had become three branches of the American religion. By American religion, Herberg meant something quite unlike traditional Western religion. It lacked a common theology and intense emphasis on faith in a transcendent power and the unseen. Instead, it was more akin to a national ideology that American democracy stood for the brotherhood of man and the dignity of the individual human being. This shared belief system was why no one was expected to change her religion as she became an American aand why the nominal differences constituted the way one claimed distinctiveness. “All other forms of self-identification and social location are either (like regional background) peripheral and obsolescent or else (like ethnic diversity) subsumed under the broader head of religions community.”[^21] President Dwight Eisenhower punctuated the point when he declared that “our government makes no sense unless it is founded in a deeply felt religious faith --- and I don’t care what it is.”

[^21]:	Herberg, _Protestant, Catholic, Jew_, 38-39.

Herberg bemoaned the lack of religious seriousness of the American religion. Rather than confronting ultimate questions, it devolved into exaggerated judgments of others’ behavior. Religion as morality was the major impulse of the growing use of the phrase “Judeo-Christian tradition,” which signified the centrality of the commonalities among the three religions. It is hard to imagine nineteenth-century religious leaders accepting those similarities were more important than the differences, forming the National Conference of Christians and Jews, or celebrating Gandhi as a moral paragon rather than labeling him a pagan.[^22] Herberg drew special attention to shifts in the Catholic Church. The Church might proclaim itself the one true church, but “in their actual social attitudes American Catholics . . . tend to think of their church as a denomination existing side by side with other denominations in a pluralistic harmony.”[^23] As William Halsey notes, many Catholics believed that the Church's concept of natural law was compatible with the Founders'.[^24]

[^22]:	Alan Petigny, _The Permissive Society: America, 1941-1965_ (New York: Cambridge University Press, 2009), 82-86.

[^23]: Herberg, _Protestant, Catholic, Jew_, 91.

[^24]:	William Halsey, _The Survival of American Innocence: Catholicism in an Era of Disillusionment, 1920-1940_ (Notre Dame, Ind.: University of Notre Dame Press, 1980).


Religious prejudices and restrictions in turn diminished. Protestants devoted less concern to converting Jews while organizations that formerly restricted or prohibited their entrance eased opposition. The American Historical Society, long a bastion of Protestant practitioners, elected its first Jewish president in 1953, Louis Gottschalk of the University of Chicago. Jewish authors, even those who plumbed Jewish subject matter and wrote with an undisguised Jewish sensibility that looked askance at the world, received terrific reviews and plaudits as leading American authors. For all their differences, Norman Mailer, Joseph Heller, Philip Roth, and Bernard Malamud were often applauded for their ability to sharpen American literature with protagonists who were neurotic, self-involved, and deeply aware of both their own limitations and the curse of conformity.[^25]

[^25]:	For an insightful recent discussion of the postwar rise of prominence of Jewish male authors, see George Hutchinson, _Facing the Abyss: American Literature and Culture in the 1940s_ (New York: Columbia University Press, 2018), 159-95.

Jews were a small percentage of the American population, perhaps three percent of the population in 1950. The more compelling case for the triple melting pot was that of the more numerous Catholics. Once shunned as superstitious and insular agents of the antichrist in Rome, Catholics basked in a new acceptance as full-fledged Americans who shared the values and customs of their fellow Americans. Prior to the war, even someone as tolerant as Margaret Mead regarded Catholics as deliberately insular. After the war, Protestants were more likely to credit Catholic desires to join the mainstream.  Join they did, achieving prominence in all walks of life. Ed Sullivan was the nation’s premier impresario on the new medium of television with his popular variety show; Jackie Gleason’s _Honeymooners_ succeeded in large measure because it captured working-class tenement life familiar to many viewers. Perry Como, Frank Sinatra, and Dean Martin dominated popular music. The entertainers did not rely on explicit Catholic content but they made no effort to explicitly deny or disguise their faith. More recognizably Catholic were the heroes of _On the Waterfront_, the broken boxer turned longshoreman Terry Malloy (Marlon Brando) who informs on corruption and murder on the Red Hook docks and Father Barry (Karl Malden) whose assurance that Christ was present eventually persuades Malloy to protect his soul.

Just as fascinating was the role of Bishop Fulton J. Sheen, whose television show _Life is Worth Living_ was so popular that it provided significant ratings competition with the perennial leader, the comedy-variety show _Milton Berle_. Sheen attracted viewers of all denominations in part because his discussions had no explicit Catholic doctrine and addressed matters of love and family. Still, as James T. Fisher notes, the general appearance of non-denominational Christianity was deceiving. Sheen masterfully drew upon Aquinas to challenge the chaos and meaninglessness of modern life.[^26] Sheen had come to prominence on the radio in the 1930s with _The Catholic Hour_ on NBC and had made a name for himself when he told his listeners that fighting fascism was not only a political imperative but a theological one on the grounds that fascism was the movement of the Anti-Christ. Not surprisingly, he made similar claims during the Cold War. The Catholic hierarchy felt similarly, pitching the struggle as one with godless Communism, thereby easing the way for great Catholic integration into American life after years of Protestant hostility. The ability of Christians of all stripes to oppose Communist regimes in turn lessened Catholic insistence that Protestantism was a reign of error.[^27]

[^26]:	James Terence Fisher, _Communion of Immigrants: A History of Catholics in America_ (Oxford; New York: Oxford University Press, 2008), 116-35.

[^27]: Jason Stevens, *God-fearing and Free: A Spiritual History of America's Cold War* (Cambridge: Harvard University Press, 2010).

John F. Kennedy was no theologian but his election as President pointed to the conundrums of Catholic assimilation. Still suspected by many Protestants, Kennedy took the prejudice head on, first in the West Virginia primary and later in the general election. In a famous September address to the Southern Baptist convention in Houston, Kennedy asserted Catholics’ allegiance to American values and the right to participate fully in civil and social life. He affirmed his belief in the absolute separation of Church and State and decried religious tests for office. Such acts would rip apart “the whole fabric of our harmonious society,” something that would weaken the country in the midst of the Cold War. Most Catholics approved of the sentiments but a significant number were wary that Kennedy had so thoroughly divided public and private that his Catholicism seemed totally incidental. Wrote the Jesuit editor of _America_, “a man’s conscience has a bearing on his public as well as his private life.”[^28]

[^28]:	Quoted in Fisher, 131.

Influential as it was, Herberg’s book attracted some meaningful dissent. Critics pointed out that American religion had not fully transformed into the mushy civil religion that Herberg disliked. He did not address fundamentalist Protestantism, which retained the allegiance of large numbers of Americans throughout the period. Far from the bloodless moral creed that Herberg discerned in mainstream Protestantism, fundamentalist denominations continued to believe in Biblical inerrancy, retained suspicion of the antichrist in Rome, and tended toward apocalyptic thinking.[^29] Although by no means as prominent in their respective faiths as fundamentalists were in Protestantism, Catholics and Jews each had substantial orthodox minorities.

[^29]:	Andrew M. Greeley, _Ethnicity, Denomination, and Inequality_ (Beverly Hills: Sage Publications, 1976); Robert S. Ellwood, _1950, Crossroads of American Religious Life_ (Louisville: Westminster John Knox Press, 2000); Robert Wuthnow, _After Heaven: Spirituality in America Since the 1950s_ (Berkeley: University of California Press, 1998).

Commentators also took issue with Herberg’s position that ethnicity—understood as connections based on common national origins—lost much of its potency in postwar mass society. Rather than disappearing, Nathan Glazer and Daniel Patrick Moynihan suggested in _Beyond the Melting Pot_ (1963), ethnicity was changing. They maintained that the word first appeared in American dictionaries in 1953 signifying a new reality. “Ethnicity,” they held, “is the steady expansion of the term ‘ethnic groups’ from minority and marginal subgroups at the edge of society—groups expected to assimilate, to disappear, to continue as survivals, exotic, or troublesome—to major elements of a society.”[^30] Ethnic groups “were recreated as something new, but still as identifiable groups.” They were “not a survival from the age of mass immigration but a new social form.” Many neighborhoods and work groups still remained ethnically homogenous. As Joshua Zeitz has demonstrated, many New York city enclaves continued to speak the language of the homeland at home and at neighborhood businesses well until the late 1950s. Similarly, fraternal and charitable organizations continued to structure life for many second and third generation ethnics.[^31] In addition, the Catholic preference for parochial schools solidified ethnic divisions. Glazer and Moynihan concurred with Herberg that religion would provide the basic divisions of identity among white Americans in the future, but it had not quite happened in 1963.

[^30]:	Nathan Glazer and Daniel P. Moynihan, _Beyond the Melting Pot; the Negroes, Puerto Ricans, Jews, Italians, and Irish of New York City_ (Cambridge: M.I.T. Press, 1963), 5, 7.

[^31]:	Joshua Zeitz, _White Ethnic New York: Jews, Catholics, and the Shaping of Postwar Politics_ (Chapel Hill: University of North Carolina Press, 2007), 11-38.



### The Struggles for Inclusion


If ethnic Americans had by and large moved to the center of American life, racial minorities had not. Conventional white liberal wisdom recognized the problems that plagued racial minorities, but insisted that Puerto Ricans, Negroes, and Mexicans would eventually become fully integrated, especially if they adopted mainstream cultural patterns particularly with regard to family life. Glazer and Moynihan laid as much responsibility on the distortions introduced into black life by the prominence of families headed by female, which they claimed dated from slavery, as outright discrimination. Prejudice, they held, was irrational and would fade in due course.[^32]

Schooled by experience, racial minorities were less sanguine. Many Puerto Ricans told stories of discrimination in education, housing, and employment despite knowing English and possessing excellent credentials. The debate between the promise of assimilation and the reality came memorably through in “America” from _West Side Story_. “Life can be bright in America” is answered by “If you’re all white in America.” The persistence of race-based exclusion prompted formation of such new organizations as the Congress of Racial Equality (1942), Southern Christian Leadership Conference (1957), Puerto Rican Forum (1957), and the American G.I. Forum (1948) to supplement the older National Association for the Advancement of Colored People (1909) and the League of United Latin American Citizens (LULAC) (1929) to achieve full citizenship rights. Glazer and Moynihan might have dismissed protest as "shrill" and "ineffective" (84); people of color did not.

[^32]:	Glazer and Moynihan, 47-51.

One intellectual tradition that people of color and their allies challenged was race talk, which posited a hierarchy based on different innate capacities. Both black and white liberals explained racial differences not by biology but by history and environment. The 1951 UNESCO statement on race, written with contributions from anthropologist Ashley Montagu and sociologist E. Franklin Frazier, boldly proclaimed that there existed no credible scientific knowledge that one human group was innately intellectually or emotionally superior or that human groups differed in their inborn capacity for development.[^33] Nearly all postwar studies of race began from Swedish economist Gunnar Myrdal’s _American Dilemma_ (1944). Chosen to lead the Carnegie Corporation study because the Corporation president thought he would be less biased than a native-born American, Myrdal initially found black people strange, particularly their emotion-filled religious services. In time, however, he realized that “in their basic human traits, the Negroes are inherently not much different from other people.” As such, their differences from “other people” resulted from white refusal to treat blacks equally. “It is . . . the white majority group that naturally determines the Negro’s ‘place.’ All our attempts to reach scientific explanations of why the Negroes are what they are and why they live as they do have regularly led to determinants on the white side of the race line.”[^34] White beliefs about black folk were often the product of myth and tradition, not impartial observation. By itself, knowing the “facts” of Negro life would not change white attitudes, Myrdal maintained. Yet he remained hopeful that exclusion and hatred would disappear, pointing to the decay of caste theory and black refusal to be a patient, submissive minority. Eventually, he posited, the conflict between the high American ideals of freedom and equality and discrimination would become too difficult to maintain psychologically and socially and would resolve itself in favor of the ideal. “The bright side [of the American dilemma] is that the conquering of color caste is America’s own innermost desire.”[^35]

[^33]:	The statement was controversial in the scientific community on grounds that Montagu arbitrarily dismissed possibilities of mental difference without sufficient testing or explanation. Physical differences clearly existed, critics charged, so mental and temperamental ones might also. Nadine Weidman, “An Anthropologist on TV: Ashley Montagu and the Biological Basis of Human Nature, 1945-1960,” in _Cold War Social Science: Knowledge Production, Liberal Democracy and Human Nature_, ed. Mark Solovey and Hamilton Cravens (New York: Palgrave MacMillan, 2012), 220-21; Perrin Selcer, “Beyond the Cephalic Index: Negotiating Politics to Produce Unesco’s Scientific Statements on Race,” _Current Anthropology_ (2012).

[^34]:	Myrdal, _An American Dilemma_, xlvii.

[^35]:	Myrdal, _An American Dilemma_, 1021.

Postwar movies followed suit in rejection of racial hierarchy. Generally not a subject of Hollywood productions of the 1920s and 1930s, racial problems formed the basis of a quartet of major films of 1949: _Intruder in the Dust_, _Lost Boundaries_, _Home of the Brave_, and _Pinky_.[^36] Begun as a Broadway play, _Home of the Brave_ switched its main character from Jewish to black because studio heads decided “Jews have been done.” Sent out with both a long-time friend and a bigot to map a Japanese-held island, the black engineer, Peter Moss, becomes paralyzed when his friend dies in his arms. Carried to safety by the bigot, which only intensifies his paralysis, Moss regains control of his body when the attending psychiatrist yells a derogatory racial term. That word breaks the spell cast by Moss’s internalization of racism, and he comes to realize he is no different from anyone else. Stanley Kramer produced the film and plumbed same territory more tendentiously with _The Defiant Ones (1958)_, the story of two escaped chain-gang convicts one black (Sidney Poitier) and one racist (Tony Curtis). Over the course of the film, the two learn to recognize each other’s humanity and cooperate with one another, even after they break their chains. Chased by a posse, their only hope of escape is to jump a freight train. Poitier hops aboard but cannot lift up the straggling Curtis. Rather than ride by himself, he jumps back down and waits with Curtis for re-capture.

[^36]:	The following discussion of the films is indebted to Thomas H. Pauly, “Black Images and White Culture During the Decade Before the Civil Rights Movement,” _American Studies_ 31, no. 2 (1990), 102-20.

The politics of Elia Kazan’s _Pinky_ (1949), the third highest grossing film of the year, were less certain. Pinky, a young, light-skinned nurse played by the white actress Jeanne Crain (chosen over both Lena Horne and Dorothy Dandridge), comes South to visit her illiterate laundress grandmother, Dicey (Ethel Waters). She has passed for white in the North and even has fallen in love with a white doctor, Tom Adams (William Lundigan), who knows nothing of her racial identity. In the South, Pinky is constantly harassed by police and hooligans. Requested by a black doctor to stay and train nurses, she initially declines. When her grandmother pleads with her to care for Dicey’s ailing white neighbor and friend Miss Em (Ethel Barrymore), Pinky reluctantly agrees. She had long disliked Miss Em, but their common humanity enables them to strike up a friendship. That friendship leads Miss Em to leave her land to Pinky in her will, which is promptly challenged. Despite all odds Pinky prevails. When Tom tracks her down and asks her to sell the land and come north passing as white, she refuses and devotes her effort instead to a clinic and nursery school for black children.

At the time the movie provoked controversy because of the mixed-race embrace and kiss between Pinky and her white suitor, who scandalously does not drop her when he learns of her heritage. Efforts to screen the film in Texas resulted in state prohibition, leading to a landmark Supreme Court case, _Burstyn v. Wilson_ (1952) that extended First Amendment protections to film. The New York _Times_ film critic Bosley Crowther appreciated the depiction of the horrors of racism but ultimately felt the film was “paternalistic” because its stereotypes and its inability to envision any resolution to racial problems other than passing or sticking to one’s own kind. Writing in the _Chicago Defender_, the NAACP's Walter White railed against the stereotypes of the kindly plantation mistress and the loyal house servant and a general attitude in which black people were simply victims of white power. That passivity, White maintained, was not true to how black people lived.

Like cinema, postwar professional sport aimed for color blindness --- at least in terms of eligibility. Although blacks and Hispanics had entered competition with whites in boxing and scattered track and field events prior to the war, professional sports remained segregated. Exclusion seemed particularly significant in Major League Baseball. Then considered the national pastime that embodied American values, it was venerated as the athletic contest that best combined sheer physical skill and mental acuity. Celebrated as an avenue of Americanization, the game that introduced immigrants and their descendants to American mores, baseball had opened up to Italians (Joe DiMaggio) and Poles (Al Simmons, nee Aloisius Szymanski) during the 1920s and 1930s. White owners, white sports writers, and white fans worried that black players would destroy the purity and legitimacy of the game and upend team chemistry. Major league baseball remained lily-white until the Brooklyn Dodgers recruited UCLA football star and Negro League shortstop Jackie Robinson to break the color barrier. Robinson’s skill, remarkable self-control in the face of unfathomable racist abuse from spectators and opponents, and eventually acceptance from his teammates especially Kentuckian Pee Wee Reese insured the success of the experiment, as the phrase went, and increased the visibility of group mixing in one of the country’s most symbolic endeavors. Other sports soon followed suit. By 1963, teams took it as a given that their success depended upon discovering and signing stars from different racial groups. Black and Latino athletes belonged, albeit under the burden of stereotypes that they were “natural” athletes who did not always have the most robust work ethics. Even more damaging was the oft-whispered sentiment among coaches and managers that they were ill suited for positions that allegedly required intellectual acumen (pitchers and catchers and football quarterbacks). Nonetheless, the success in sports was often trotted out as a model of opened doors for other endeavors to follow.

Not surprisingly, Robinson and Brooklyn were easily drafted as a symbol of a multi-ethnic and -racial America united by brotherhood. Bette Bao Lord’s 1984 children’s novel, _In the Year of the Boar and Jackie Robinson_, makes the point eloquently. Excited to be joining her father in the United States in late 1946, a young girl and her mother journey from China to Brooklyn. In anticipation of a new beginning, she chooses a new American name, Shirley Temple Wong. Not knowing English or American ways, she struggles less with school work than with fitting in. Eventually turning to the radio, she begins to follow the exploits of the Dodgers in general and Robinson in particular. As his struggles mirror hers, so too does his successes. Her interest in the Dodgers and baseball is shared, and she even learns to do make a stab at stickball. She thus becomes part of the Brooklyn mosaic formed by black, Italian, and Jewish classmates. Putting the seal on her acceptance was her selection to give the key to her school to Robinson himself.

_In the Year of the Boar and Jackie Robinson_ is a loosely fictionalized account of Lord’s own American experience and seems romanticized. Shirley’s classmates easily come to regret their casual racism when they actually get to know her. Midcentury was less hospitable to Chinese-American both in fictional portrayals and the culture as a whole. Long the subject of exclusion and oppression, Americans of Chinese descent were subjected to vicious stereotypes and, prior to the postwar period, regarded as clannish and unassimilable. The idea of unbridgeable difference gradually eroded during midcentury. Aided by the end of the Exclusion Act in 1943, the number of Chinese-Americans reached 237,000 by 1960. The growth of Chinese-American communities opened questions about whether they would become full-fledged members of American society. The dilemmas of Chinese assimilation were at the heart of a best-selling novel, and a popular Richard Rodgers and Oscar Hammerstein II musical and subsequent movie, _Flower Drum Song_. Written by C. Y. Lee, the novel tells the story of a rich patriarch, Master Wang, who arrives in San Francisco’s Chinatown having fled the Communists. There he tries to live by traditional Chinese ways, particularly in insisting that deference in properly paid to elders, especially when they make marriage and career choices for their children. Immigration restrictions limited the number of available women, and created both a “bachelor society,” probed as well by Louis Chou’s _Eat a Bowl of Tea_ (1961) and something akin to a marriage market based on supply and demand. Wang strikes a bargain for an arranged marriage for Ta, his son, Ta rejects the arranged marriage and declares his love for a woman from a lower class. Eventually Old Wang defers to the American mode of individualism as more likely to bring family happiness and even accepts the superiority of Western medicine over Chinese herbalist.

The musical and movie change the story line by dampening the licentiousness of bachelor life, the class conflicts, and the economic nature of marriage. It does share the preference for assimilation, even including a song “Chop Suey” that celebrates American diversity. The Chinese assimilation depicted in _Flower Drum Song_ did not entail anything so radical as mixing on equal terms with whites, who are very much an implied presence. Although the play and movie flattered white audiences with its validation of American culture, its popularity rested on the pageantry of musical numbers and the frisson in the presentation of exotic people. Small wonder it has subsequently been dismissed as patronizing.

Postwar public opinion turned toward inclusiveness in part as a recoil from Nazi genocide and in part as a recognition that American apartheid put the United States at a marked disadvantage with the peoples of the decolonizing world. Gradually politicians understood the need to dismantle the system of exclusion. Particularly notable in the growth of sentiment to redress the failure of American ideals was the speech of Minneapolis mayor Hubert Humphrey to the 1948 Democratic Convention. Challenging the majority report of the platform Committee that had deferred to white Southern sentiment, Humphrey forcefully argued for the morality and political advantage endorsing Harry Truman’s civil rights program. Humphrey prevailed and was thrust into the heart of national politics.[^37]

[^37]:	Jennifer A. Delton, _Making Minnesota Liberal: Civil Rights and the Transformation of the Democratic Party_ (Minneapolis: University of Minnesota Press, 2002).

Most important, however, was consistent pressure from people of color to effect change. Both African- and Mexican-Americans built their postwar efforts on initiatives undertaken during the war. LULAC was the more conservative of the Mexican-American organizations. Established in Corpus Christi, it emphasized not, as previous groups had, alliances of Mexican nationals living in the United States and American citizens of Mexican descent, but the rights of those of Mexican descent who were citizens. That emphasis was particularly noteworthy during the Depression when 500,000 Mexicans (including some with citizenship) were deported. Part of the strategy to secure citizenship rights were efforts to convince the Anglo majority that Mexican-Americans were no different from their fellow citizens. The leadership stressed Mexican-American “whiteness” and fostered assimilationist practices, particular English language use. LULAC opposed open immigration from Mexico, worried that newcomers, especially those in the _bracero_ program (1942) that allowed farm workers to enter temporarily to ease agricultural labor shortages, would not be assimilable. The G. I. Forum, which was originally founded to secure veterans’ benefits for soldiers of Mexican descent, rejected LULAC’s emphasis on whiteness because it enabled the exclusion of Mexican-Americans from juries in trials of those of Mexican descent. The Forum instead favored a pan-Mexican organizing strategy. LULAC’s assimilation philosophy did enable the organization to push forward with school desegregation cases. Its victory in _Mendez v. Westminster_ (1947) established a basis for the argument that segregation by the very separation of groups conferred inferiority.[^38]

[^38]:	Benjamin Marquez, _LULAC: The Evolution of a Mexican American Political Organization_ (Austin: University of Texas Press, 1993), 26-32, 51-4.

The issue returned in slightly different form in the most celebrated court case of the period, _Brown v. Board of Education_ (1954).[^39] As the lawyers for the NAACP Legal Defense fund took on legally sanctioned separation of the races, they had needed to demonstrate that segregation resulted in unequal facilities and personnel and that enforced separation of black and white pupils marked black students as inferior and placed them at a considerable disadvantage. To argue against the controlling decision _Plessy v. Ferguson_ (1896), in other words, advocates needed to show how and why separation denied one group the possibility for full enjoyment of their capacities. Unlike the lawyers in _Mendez_, those in _Brown_ could not attempt to argue that the plaintiffs had been wrongly classified as nonwhite.

[^39]:	Among the discussions of _Brown_ consulted were James T. Patterson, _Brown V. Board of Education: A Civil Rights Milestone and Its Troubled Legacy_ (Oxford; New York: Oxford University Press, 2001); Mark V. Tushnet, _The NAACP’s Legal Strategy Against Segregated Education, 1925-1950_ (Chapel Hill: The University of North Carolina Press, 2005); Michael J. Klarman, _Brown V. Board of Education and the Civil Rights Movement_ (New York: Oxford University Press, 2007); Richard. Kluger, _Simple Justice: The History of Brown V. Board of Education and Black America’s Struggle for Equality_ (New York: Knopf, 1976); Risa Lauren Goluboff, _The Lost Promise of Civil Rights_ (Cambridge, Mass.: Harvard University Press, 2007); Waldo E. Martin, “Shades of _Brown_: Black Freedom, White Supremacy, and the Law,” in _Brown V. Board of Education: A Brief History With Documents_, ed. Waldo Martin (Boston: Bedford/St. Martin’s, 1998).


That black and white schools were unequal was not so much in dispute in the mid-century debates. State legislatures with separate school districts tacitly acknowledged the blatant inequality in facilities in the 1940s by dramatically increasing spending on all-black schools in order to preserve _de jure_ segregation. Operating from the assumption of absolute incompatibility of the races, segregationists justified segregation as a matter of private racial preferences for people of similar nature. Separation was not, they insisted, intended to reinforce inequality. Any inferiority attached to separate school systems was psychological, something black people imagined. Fund lawyers rejected the argument, contending that separation only made sense if one believed that differences between races were both meaningful and fixed. Since they were neither, segregation constituted exclusion and conferred inferiority. In a revealing confession of the NAACP focus on ending exclusion, Robert Carter, an assistant to lead counsel Thurgood Marshall, later admitted that members of legal team regarded segregation itself as the fundamental evil and not as a “symptom of the deeper evil of racism.”[^40] They believed schools were an entering wedge to bring down the entire edifice. Once separation was ended, black performance would improve and acceptance would follow.

[^40]:	Quoted in Patterson, _Brown V. Board of Education_, 41.

As Risa Lauren Goluboff has argued, the NAACP might have attacked Jim Crow not by appealing to formal equality and psychological harms but by stressing economic harms and labor rights. Building on New Deal economic protections in private workplaces, the alternative strategy would have used the Thirteenth Amendment to challenge economic coercion. If successful, it would have given the black freedom struggle a stronger economic component that would have allowed decent wages and right to organize for agricultural and domestic workers who were excluded from many government programs. The NAACP flirted with the approach but put full effort in all-out effort to overturn government discrimination and reverse the inferiority and stigma caused by segregation. To demonstrate psychological harm, lawyers turned to a controversial social-psychological study by Kenneth and Mamie Clark that purported to show that segregation schools lowered black children’s self-esteem. Segregationists railed against the substitution of psychology for law, conveniently overlooking their own reliance on the psychology of social similarity. In the end, Chief Justice Warren accepted that separation was tantamount to exclusion, arguing in footnote eleven that segregation created a sense of inferiority and deprived black children “of some of the benefits they would receive in a racial[ly] integrated school system.”[^41]

[^41]:	Arguments for the labor strategy can be found in Goluboff, especially 237-65. _Brown v. Board of Education_, 347 U.S. 483, 494.


Fund supporters overestimated the degree to which _Brown_ would usher in a colorblind, inclusive society in which race ceased to matter. In part, the Court miscalculated when it ordered districts to dismantle their dual school systems “with all deliberate speed,” which segregationists took to mean “slow, if ever,” rather than immediately. In part, liberal hopes to the contrary, whether for economic, political, or psychological reasons, many whites were not ready to abandon white supremacy for colorblindness. The dawning recognition that resistance was massive provoked a new urgency among activists, who turned more widely to the more confrontational tactics of sit-ins, boycotts, and marches. It also led some to emphasize self determination for people of color rather than formal legal equality.
Liberals also overestimated their own colorblindness. Many of the standards said to be universal turned out on inspection to be white and middle-class. Such was the case with the work of historian Kenneth Stampp. Stampp’s _Peculiar Institution_ (1956) was a direct challenge to the work of racist historian Ulrich B. Phillips, particularly _Life and Labor in the Old South_ (1929). Phillips asserted that slavery civilized a barbarous people and protected them even at the cost of the economic well-being of the masters. Stampp, to the contrary, saw slavery itself as cruel and inimical to the welfare of the slaves. As for the essential racial character, Stampp, in a line that he would be forced to explain a decade later when a different ideological inclination prevailed, averred “slaves were merely ordinary human beings, that innately Negroes are, after all, only white men with black skins, nothing more, nothing less.”[^42] Stampp’s use of the appositive “after all” here signals that his assumption that it was common sense that at base all peoples are similar. In later editions of his _magnum opus_, Stampp added a paragraph indicating that, of course, he recognized differences in culture but wanted to reject the sense that cultural differences meant blacks lacked the ability to be equal participants in American life. For his purposes, he noted, he could just as well have written that Caucasians were black people with white skin.

[^42]:	Stampp, _The Peculiar Institution: Slavery in the Ante-Bellum South_, vii.

Although he might have put it that way, it was telling that he did not. Stampp was not the only white liberal to ignore black culture. Glazer and Moynihan, for instance, contended that the “Negro is only an American,” who had no special values or culture to protect.[^43] Milton Gordon claimed based on interviews that the leadership of many mainstream African American organizations did not “envision the retention of a Negro subcommunity with its own institutions as a desirable long-range goal for Negroes in the United States.”[^44] Others followed Myrdal in emphasizing the supreme importance of slavery and segregation in defining black life. Historian Stanley Elkins envisioned slavery as sharing with Nazi death camps a tendency to dissolve all previous identity and culture. Anthropologists Abram Kardiner and Lionel Ovesay posited that whites and blacks shared the same culture and goals. Racism frustrated black attainment of those goals, which resulted in black males neurotically exaggerating white personality traits.[^45]

[^42]:	Glazer and Moynihan, _Beyond the Melting Pot_, 53.

[^44]:	Gordon, _Assimilation in American Life_, 14.

[^45]:		Stanley M. Elkins, _Slavery; a Problem in American Institutional and Intellectual Life._ (Chicago: University of Chicago Press, 1959); Abram Kardiner and Lionel Ovesey, _The Mark of Oppression: A Psychosocial Study of the American Negro_ (New York: Norton, 1951).


Many black intellectuals and artists, even those who campaigned vigorously for integration, recoiled from the dismissal of blackness. Such figures as Ralph Ellison, Albert Murray, James Baldwin, and Lorraine Hansberry knew black people were not simply white people with black skins who lacked a culture of their own. Throughout the period in essays and in art they revealed, sometimes inadvertently, black mores and values. Their recognition of black distinctiveness did not, however, entail agreement about the future they desired. “Militant integrationists” in the words of Henry Louis Gates, Murray and Ellison saw in the blues an outlook that could be one the building blocks of a reconstituted American culture.[^46] Baldwin and Hansberry, on the other hand, were considerably more dubious about American ability to include people of color on anything approaching meaningful terms and while never full-blown separatists, admired black rebelliousness and independence.

[^46]:	Henry Louis Gates, “King of the Cats,” _The New Yorker_ (1996).

Ellison’s criticism of racial liberalism appeared in his unpublished review of Myrdal. Praising the book’s demonstration that “typical” Negro traits were not innate, he nonetheless objected to its over-reliance on the “sterile” concept of race that made it all too easy to assume there was a psychological barrier (by which Ellison meant affinities and ideas about each other) between the two groups. Ellison took exception to Myrdal’s portrait of Negro culture as solely reactive to white power and the typical Negro personality as the product of “social pathology.” Myrdal failed to acknowledge Negroes’ initiative and their ability to create themselves. It is, Ellison insisted, “only partially true that Negroes turn away from white patterns because they are refused participation. There is nothing like distance to create objectivity, and exclusion gives rise to counter values.” Although Myrdal had maintained that it was to the advantage of American Negroes to assimilate and acquiring traits dominant whites valued, Ellison countered that the American culture to which Negroes had to assimilate also had such pathologies as lynching, Hollywood movies, fadism, and radio advertising. Ellison saw unappreciated value in black music, folklore, cuisine, dance, dramaturgy, and a tragic comic sense found only in the blues. Negroes, he claimed, “will not willingly disregard” their heritage for the sake of integration. Nor should whites, who could join blacks in a mutual undertaking that would help “create a more human American.”[^47]

[^47]:	Ellison, “Shadow,” 339-41.

Ellison’s masterpiece, _Invisible Man_ (1952) built upon his reaction to Myrdal. Rejecting the social realism of the protest novel, which to his mind reduced complex people to the sum total of social forces, Ellison aimed at a vital depiction of the quest for identity of his unnamed narrator, who is both “black and American.” The book certainly depicts outright racism in the Battle Royal scene in which black school boys engage in a rumble for the enjoyment of whites in the narrator’s Southern hometown and in the degrading stereotypes of the culture industries (the Sambo dolls being among the most notable example in the novel). Its main thrust, however, is the narrator’s engagement with the movements and ideologies that circulated among black people after Emancipation. Designed to demonstrate Negro initiative rather than Myrdalian passivity, written with “blues-toned laughter,” the novel as its goal “revealing the human universals hidden within the plight” of his narrator, creating grounds for white identification with black sensibilities. Ellison took the narrator through Washington self-help embodied in the Tuskegee-like college experience, the Marxism of the white-dominated Brotherhood, and the black nationalism of Ras, the Destroyer.[^48] Each provide a limited identity for the narrator and each fails to deliver on its promises. Self-help fails when the authoritarian college president Bledsoe punishes the narrator for showing a white liberal donor the underside of black life, the dissolute Jim Trueblood and the saloon filled with the bitter and traumatized black veterans of World War One. The Brotherhood’s Brother Jack preaches solidarity but reveals himself to be interested only to the degree he can manipulate embittered residents of Harlem. Ras’s hatred extends not only to whites but to blacks who work with them. Only by rejecting these false visions can the narrator be visible to himself.[^49]

[^48]:	Ralph Ellison, “Introduction,” _Invisible Man_ (New York: Vintage Books [1952] 1981), xviii, xvi.

[^49]:	Among the interesting views on the novel are Saul Bellow, “Man Underground: A Review of Ralph Ellison’s _Invisible Man_,” _Commentary_ (1952); Tracy Floreani, _Fifties Ethnicities : The Ethnic Novel and Mass Culture at Midcentury_ (Albany: State University of New York Press, 2013); Jay Garcia, _Psychology Comes to Harlem : Rethinking the Race Question in Twentieth-Century America_ (Baltimore: Johns Hopkins University Press, 2012); Lawrence Patrick Jackson, _The Indignant Generation: A Narrative History of African American Writers and Critics, 1934-1960_ (Princeton: Princeton University Press, 2011); An early effort to collate Ellison criticism is Alan Nadel, _Invisible Criticism: Ralph Ellison and the American Canon_ (Iowa City: University of Iowa Press, 1988); The definitive biography is Arnold. Rampersad, _Ralph Ellison: A Biography_ (New York: Alfred A. Knopf, 2007).

Not that invisibility lacks advantages. At points the narrator courts invisibility as when he dons glasses and a hat and is taken to be the shape-shifting Rinehart. Invisibility allows him to travel unrecognized and unencumbered by obligations. Yet it eventually proves no less fulfilling. Chased by Ras during the Harlem riots, he escapes by plunging down a manhole. As the novel ends, he is residing in a portion of a basement that no one else seems to know exists, illuminated by 1,369 lights with power siphoned off from the power company, smoking marijuana and listening to Louis Armstrong. He eventually realizes that while no one can define him, humans need others. His self-chosen withdrawal is not a legitimate response to those forces of conformity that prevent humans from keeping their “many parts.” Becoming visible does not mean becoming white. Early on while working for Liberty Paints, the narrator learns that the company’s famous Optic White is produced by adding ten drops of black paint, a testimony to the hybridity of American life. In conclusion, the narrator echoes the point: “America is woven of many strands; I would recognize them and let it so remain. It’s ‘winner take nothing’ that is the great truth of our country or of any country. Life is to be lived, not controlled; and humanity is won by continuing to play in the face of certain defeat. Our fate is to become one, and yet many” (576).

Even whites who were familiar with black culture often misunderstood it as primitive. The most infamous venture in racial borrowing among intellectuals was Norman Mailer’s “White Negro.” The essay joined the bohemian spirit to black culture, lauding the ways in which both Beats and blacks stood against the conformist, “totalitarian tissues” of American society. In Mailer’s telling, Negroes have been living on the margin, embracing danger and had no need for the “sophisticated inhibitions of civilization.” Praising blacks’ ability to live in the enormous present, indulging in “Saturday night kicks,” and relinquishing “the pleasures of the mind for the more obligatory pleasures of the body,” Mailer championed the ability to give voice to joy, lust, and despair in jazz. “For jazz is orgasm, it is the music of orgasm, good orgasm and bad. . . communication by art because it said ‘I feel this, and now you do too.’”[^50]

[^50]:	Norman Mailer, “The White Negro,” _Dissent_ (Fall, 1957): https://www.dissentmagazine.org/online\_articles/the-white-negro-fall-1957, accessed 26 June 2017.

Black intellectuals found the alliance Mailer proposed unappealing, since it cast black folk as creatures of id, who lacked bourgeois restraint and a work ethos. Ellison complained to his friend Albert Murray that Mailer, deluded that all “hipsters are cocksmen possessed of great euphoric orgasms,” had placed “the same old primitivism crap in a new package.” James Baldwin observed Mailer was a poseur. All jazzmen he knew thought Mailer was a nice Jewish boy posing as an outlaw. Baldwin’s friend, the dramatist Lorraine Hansberry (1930-1965), gave credit to Mailer’s opposition to conformity and prudery, but criticized him for his misunderstanding of the complex nature of black culture. He was, she maintained, very much a “New Paternalist” who reworked old slurs to show liberation from “the hanky-panky of liberalism.” Endowing blacks with special sensuality, Hansberry continued, allowed Mailer to reduce Negroes to their oppression rather than recognize their full humanity. “White America has to believe ‘The Blacks are different --- and not only so, but that, by the mystique of this difference, they actually profit in certain charming ways which escape the rest of us with all our engrossing complexities.’”[^51]

[^51]:	Lorraine Hansberry, “Thoughts on Genet, Mailer, and the New Paternalism,” _The Village Voice_ (June 1, 1961): 10, 15.

Hansberry’s extended defense of black culture against appropriation undercut a popular view of her famous play, _Raisin in the Sun_, as a tale about universal human persistence in the face of adversity. The New York _Times_ added to that impression when it misquoted her to the effect that she regarded herself as a playwright who just happened to be a Negro. In his 1967 classic _Crisis of the Negro Intellectual_, the Marxist-turned-black nationalist Harold Cruse considered _Raisin_ the swan song of the integrationist ideal of universalism. The play, he charged, was intended for white people and aspired to have black folk conform to white ideals. Because Hansberry was the daughter of Chicago realtors who brought and won the landmark case against restrictive covenants but who later were cited for buildings below code, Cruse declared that she knew little of black suffering. Proof of her privileged position and lack of connection with the realities of black life, Cruse insisted, was that the play turns on divvying up the $10,000 life insurance policy of the deceased patriarch, Big Walter Younger. That sum, according to Cruse, was an unrealistic one for the vast majority of black people. Still others have pointed to her willingness to point to aesthetic and political lessons from classical and Western sources as indicative of her universalist, integrationist aspirations.[^52]

[^52]:	Harold Cruse, _The Crisis of the Negro Intellectual._ (New York: Morrow, 1967), 267-270, 420.

White audiences might well have found the play inspiring and, in its way, reassuring, but Hansberry was at pains to deny the play could have been about anyone. Such a formulation, she argued, flattered white liberals who talked so breezily about American equality. In fact, she told famed radio interviewer Studs Terkel that she was determined to dramatize the special black vantage point on life. Being black in America was self-evidently painful, even oppressive, but it also possessed resources of inspiration and resistance. Universality in drama, she maintained, came through “very great attention to the specific. It emerges from the truthful identity of what is.” The family at the heart of the play is definitively “first and foremost a Negro family. More precisely a Southside Chicago family for it was such a family that was formed in the specific living conditions and developed a specific set of reactions and goals as a result. What makes audiences think beyond the specifics was, ironically, must first be who they are before they can become everybody. So I would say it is definitely a Negro play before it is anything else.”[^53] _Raisin_ is filled with distinctive features of black history and culture. Walter’s sister, Beneatha, dates an African student, and adopts more natural dress and beauty standards, despite teasing from the rest of the family. When finally pushed by the “Welcoming Committee,” the younger Walter asserts family pride to refuse the pressure to leave and even fantasizes being like Kenyatta. The reference was only one of many to black revolutionaries in Hansberry’s plays.

[^53]:	Lorraine Hansberry, _To be Young, Gifted and Black_ (Vintage, 1996), 128.

Hansberry’s cosmopolitanism prevented her embracing separatism. The Nation of Islam (NOI), on the other, emerged as the premier separatist organization of the era. Founded in 1930 in Chicago, the Nation boldly advanced an origin story of black chronological primacy upset by white devils later created to suppress black achievement and freedom through lies, eugenic elimination, and outright seizure of power. The Nation preached black pride, discipline, and achievement as crucial to the establishment and maintenance of orderly communities. The Nigerian scholar E. O. Essien-Udom, who did fieldwork with the Nation, concluded that the sense of purpose NOI conveyed rather than its version of Islam accounted for its growing appeal. Essien-Udom further noted that NOI were unique among black nationalists in that it made little reference to a past culture or a common American or African history. Rather it pledged a complete reconstruction. NOI naming practices (Malcolm X being the most well-known) in which American blacks dropped names of former masters as symbolic of their old, slave past were indicative of that goal. NOI separatism was also apparent as well in the relative silence of its midcentury leader, the Honorable Elijah Muhammad, about African independence movements or the burgeoning civil rights movement in the United States. The Nation held the civil rights movement in disdain, condemning its integrationist goals as assimilationists and too dependent on a non-existent good will of white devils. Talk of love and co-existence, its members said, would never build a meaningful racial identity. Business success, the elimination of neighborhood crime, and the use of the Temple as a community center prompted growth in membership. By 1959, NOI boasted some fifty temples and enrolled followers, depending on the source, as high as 200,000. Made famous by the CBS show, “The Hate that Hate Produced,” the Nation excited white fears of violent confrontation, although NOI members never carried weapons. Although NOI rhetoric emphasized white devils, it intended not to suppress whites but to establish a homeland where blacks could determine their own destiny.[^54]

[^54]:	E. U. Essien-Udom, _Black Nationalism: A Search for an Identity in America_ (Chicago: University of Chicago Press, 1962); Theodore Draper, _The Rediscovery of Black Nationalism_(New York: Viking Press, 1970).

People of color were not the only group that had to struggle against the belief that their innate nature disqualified them from full and equal participation in American life. Women too found their exclusion from certain spheres of American life justified by their nature. As popular media constantly reminded its audiences, women were naturally more passive, emotional, and sensitive to the needs of others. Consequently, they were less capable of making rational decisions or striving for individual achievement. As such, they were suited for support roles, not leadership and nurturing rather than production.

It was the aim of Betty Friedan’s _Feminine Mystique_ (1963) to expose how this conception of female capacities and inclinations obscured women’s universal humanity and prevented their full participation in American life. A Smith College graduate, Friedan (1921-2006) won a psychology fellowship to work under Erik Erikson at Berkeley but, under pressure from her then boyfriend, abandoned Ph. D. work. She then turned to journalism, working for, among other places, the paper of the left-leaning United Electrical Workers. That position ended when she was let go after she became pregnant with her second child. She then turned to freelancing for women’s magazines and it was while surveying college graduates about the state of their post-collegiate lives for a presentation at her fifteenth college reunion in 1957 that she hit upon the topic that would form the basis of her magnum opus---the famous problem with no name.[^55]

[^55]:	Judith Hennessee, _Betty Friedan: Her Life_ (New York: Random House, 1999) and Daniel Horowitz, _Betty Friedan and the Making of the Feminine Mystique: The American Left, the Cold War, and Modern Feminism_ (Amherst: University of Massachusetts Press, 1998).

Friedan found that, despite marriage and motherhood, college-educated women were haunted by a feeling of purposeless and emptiness. This omnipresent sense of incompleteness derived from what Friedan termed the “feminine mystique.” The mystique was a near universal belief that located the very core of womanhood in creation and nurture of life, investing biological functions with a spiritual and transcendent quality. With motherhood the ultimate measure of female identity, women need only accept their nature to fulfill their destiny. The intense promulgation of the ideology, Friedan insisted, convinced women to leave their World War II-era jobs for family homes. As the dissatisfaction of her classmates indicated, however, homemaking neither brought personal tranquility nor aided the social good. By exaggerating the power of anatomy, Friedan asserted, the mystique denied woman their full status as human. The failure of domestic life to deliver the expected rewards prompted women to seek relief from the nameless frustration in self-destructive behavior. Perhaps most devastating was, Friedan claimed, was “mounting sex hunger” in hopes that constant erotic thought and behavior would convey a feeling of vitality. This stunted existence, Friedan concluded, followed from Americans’ refusal to recognize that like men, women faced tasks of self enhancement and growth. It was her hope that the dissatisfaction would be “a turning point from an immaturity that has been called femininity to full human identity.”[^56]

[^56]:	Betty Friedan, _The Feminine Mystique_ (New York: Laurel, 1983), 79.

_Mystique_ was filled with stories of intelligent women under the spell of the mystique who cut short educational and career objectives in exchange for the “Mrs.” degree. The great contribution of the book was to provide a meaningful explanation of how the refurbished ideology of domesticity was spread and internalized. Friedan mapped the wide array of institutions that operated on the premise that women’s identity was rooted in their biology and that their nature was emotional and passive. Among those she singled out were women’s magazines (more often than not edited by men for the stereotypical woman of the home), advertising that reinforced notions of childlike, oversexualized women, and psychology, especially that with a Freudian lineage. Advertising in Friedan’s view sexualized utilitarian objects, reinforcing beliefs that women’s sense of self depended upon their bodies, not their accomplishments. By affirming the incomplete female personality structure, psychology provided a supposedly unimpeachable scientific justification for the mystique. Modern-day psychological discourse put at the center of female psychology penis envy, the desire for the male organ and the power it embodied and posited that women could compensate for their incomplete nature by fully embracing motherhood. Despite their ubiquity, neither advertising nor psychology offered real satisfaction for women.

Friedan’s argument for common humanity was rooted in the alternative psychology of Erikson and humanistic psychologist Abraham Maslow. Maslow posited a pyramid of needs ranging from the physiological at the bottom to the self-actualization at the apex. Self-actualization needs, which included morality, creativity, and spontaneity, came into play when individuals engaged in a quest to achieve their full potential. Neither Erikson nor Maslow applied their theories to women or were conceived that the development that they described pertained to them (310-11). By limiting women to the lower levels of Maslow’s hierarchy, Friedan maintained, society suffered. “If women’s needs for identity, for self-esteem, for achievement, and finally for expression of her unique human individuality are not recognized by herself or others in our culture, she is forced to seek identity and self-esteem in the only channels open to her: the pursuit of sexual fulfillment, motherhood, and the possession of material things” (315-6). Friedan therefore urged women to reject deadening housework (342), reduce their investment in marriage (344), and train themselves for meaningful work. “Who knows,” Friedan asked at the conclusion of her book, “of the possibilities of love when men and women share not only children, home, and garden, not only the fulfillment of their biological roles, but also the responsibilities and passions of the work that creates the human future and the full human knowledge of who they are?” (378).

The _Feminine Mystique_ has rightly earned a place as a feminist classic --- one of those works that not only documents gender oppression but provides a coherent and meaningful account of how it occurs. The book’s staying power owes much to its incisive language that makes its critique both comprehensible and memorable. Friedan’s indictment of American gender relations struck an immediate responsive chord with her readers. Many women wrote to Friedan enthusiastically, crediting the book with giving voice to the dissatisfaction they felt and opening up possibilities to challenge what Friedan had termed the “comfortable concentration camp” of wife and mother roles.

The book was not without its limitations. Critics then and since have noted that Friedan at times both missed some of the sources of women’s exclusion and at others overstated their marginality. Friedan often let white middle-class suburban women stand in for women as a whole. By concentrating on college-educated women, she explored little of the experiences of working-class women or women of color, who were less likely to be subjected to the mystique or afford to forfeit a paycheck. Indeed, Friedan may well have overemphasized the postwar return to domesticity. Throughout the 1950s, the number of married women in the workforce grew by 42 percent. The feminist poet Eve Merriam even produced a children’s book, *Mommies at Work* (1961), to commemorate the condition.[^57] Susan Hartmann has shown that Cold War rhetoric simultaneously insisted that the family was a foundation of national defense and urged increased productivity. The first envisioned women in the home; the second implied places for them in the workforce. The National Manpower Council and the Commission on the Education of Women both recommended at one point child-care grants and supplements for higher education, particularly in the sciences.[^58] Nor could one say that the era lacked for creative woman. Their presence in no way refutes the notion of male domination of the arts but such women who came to prominence in the immediate postwar period as Hannah Arendt, Harriette Arnow, Elizabeth Bishop, Gwendolyn Brooks, Rachel Carson, Janet Flanner, Lorraine Hansberry, Margaret Mead, Mary McCarthy, Diana Trilling, Lillian Smith, Flannery O’Connor, and Jean Stafford constitute such significant contributions to postwar American letters as to complicate any notion of complete female silence.[^59] Others have pointed out that Friedan’s portrait of the intellectual climate was incomplete. For example, orthodox psychoanalytic opinion on gender and sexuality was considerably more varied than she acknowledged. Although there was a good deal of reductionism in psychoanalytic debates, there was also sectors of the analytic community that understood the domesticity in resolving psychological problems. Friedan used had Lundberg’s and Farnham’s misogynist _Modern Woman_ as representative of Freudianism, although neither author was an analyst and the _Psychoanalytic Journal_ criticized the book as retrograde and analytically confused.[^60] As historian Joanne Meyerowitz has shown in her extensive study of women’s magazines, the mainstream press offered mixed messages. Readers were just as likely as not to find articles praising female creativity and assertion, rejecting nurturing love as the highest expression of femininity, and recommending marriages based on true sharing.[^61]

[^57]:	Eve Merriam, _Mommies at Work_ (New York: Knopf, 1961).

[^58]:	Susan Hartmann, “Women’s Employment and the Domestic Ideal in the Early Cold War Years,” in Joanne Meyerowitz, _Not June Cleaver: Women and Gender in Postwar America, 1945-1960_ (Philadelphia: Temple University Press, 1994), 84-100.

[^59]:	A wonderful roster of creative women in an age that supposedly discouraged female accomplishment can be found in Eugenia. Kaledin, _Mothers and More: American Women in the 1950s_ (Boston: Twayne Publishers, 1984).

[^60]:	Eli Zaretsky, “Charisma or Rationalization? Domesticity and Psychoanalysis in the United States in the 1950s,” _Critical Inquiry_ 26, no. 2 (2000), 338-41.

[^61]:	Joanne Meyerowitz, “Beyond the Feminine Mystique: A Reassessment of Postwar Mass Culture, 1946- 1958,” in Meyerowitz, _Not June Cleaver_, 229-62.

Contrary to legend, Friedan did not singlehandedly revive feminism or solely provide its intellectual basis. Other contemporary feminists recognized the role of biology and psychology in providing justification for circumscribing female aspiration. Such prominent writers as Ruth Herschberger (1917-2014) and Eve Merriam (1916-1992) provided incisive dissents from the conventional wisdom about women’s nature and sharp arguments for their emancipation from restricted status. Like Friedan, Herschberger and Merriam were also Jewish, held advanced degrees, and had left-wing connections. Like _Mystique_, Herschberger’s _Adam’s Rib_ (1948) and Merriam’s _After Nora Slammed the Door: American Women in the 1960s, The Unfinished Revolution_ (1964) opposed motherhood as the sole natural role for women and female deference as a _modus operandi_. The books also shared a resolve to chart the institutions that restricted women’s possibilities, a project Merriam had undertaken as well with _Figleaf_ (1960), an analysis of the fashion industry’s manipulation of women.[^62]

[^62]:	Eve Merriam, *After Nora Slammed the Door* (Cleveland: World Pub. Co., 1964); Eve Merriam, *Figleaf: The Business of Being in Fashion* (Philadelphia: Lippincott, 1960); Ruth Herschberger, *Adam’s Rib* (New York: Pellegrini & Cudahy, 1948).

Given the role of biological science in defining women, both Herschberger and Merriam set their sights on how it came to its findings. They both charged that buried within supposedly objective science were numerous unwarranted assumptions. From their vantage point, studies of sex differences all too often proceeded by taking male and female qualities as given rather than things to be demonstrated. In her 1944 dismantling of Robert Yerkes’ famous work with chimps, Herschberger pointed to the loaded language and tendentious interpretation of ambiguous results that allowed Yerkes to assume incorrectly that males were naturally assertive and females depended on sexual allure.[^63] Responding to assertions that women could not do work involving abstractions because they were naturally concrete thinkers, Merriam, for her part, noted the ways in which evidence to the contrary such as tremendous success of women in theoretical physics classes when they were allowed to take them was discounted or ignored.[^64] Nor was science the only institution that misperceived women. The assumption of female difference was embedded in language itself. “Not the least of man’s capacities,” Merriam argued, “is our male-oriented language” (208). One form of asserting male dominance, Merriam wrote, was “to assign the female to her place as a non-man and set her aside” (206). Another was to incorporate women in a larger whole. The word _man_, she noted, has come to stand for both sexes. Merriam hoped that eventually American society would be organized to “make use of the full individual,” but contended that women needed to overcome the ill effects of internalization of assertions of female difference (67).

[^63]:	Herschberger, _Adam’s Rib_, 5-28. See also Shira Tarrant, _When Sex Became Gender_ (New York: Routledge, 2006), 190-210.

[^64]:	Merriam, _After Nora Slammed the Door_, 53.

For gays and lesbians, belonging was especially problematic. Facing both legal prohibition and social condemnation, most survived by not acknowledging the nature of their sexual desire. Prior to the war, few gay and lesbian communities in which there was an understanding of commonality existed. Some large cities such as San Francisco, Chicago, and New York did have special meeting places such as public baths or particular parks and distinctive customs as to how signal interest, and special roles and traits such as “fairies” or “butches.” Community-building intensified with the end of Prohibition, which made possible bar-based cultures, and World War II, which threw together large groups of like-inclined men and women who suddenly realized how prevalent their desires were.[^65]


[^65]:	George Chauncey, _Gay New York: Gender, Urban Culture, and the Making of the Gay Male World, 1890-1940_ (New York: Basic Books, 1995); Charles Kaiser, _The Gay Metropolis: 1940-1996_ (Boston: Houghton Mifflin, 1997); and Nan Boyd, _Wide-Open Town: A History of Queer San Francisco to 1965_ (Berkeley: University of California Press, 2003).

It also alerted the larger heterosexual community to their presence and prompted efforts to segregate. New laws required bars and dance halls to surveil their patrons and prohibit same-sex dancing. Other laws were designed to harass drag queens. Gays and lesbians were especially vulnerable at work. Inappropriate gender presentation and accusations of homosexual activity were grounds for dismissal. Dwight Eisenhower tried to rid the WACs of lesbians but stopped when he realized that nearly eighty percent of his female support staff would be drummed out of the corps.⁠ He did manage to issue executive orders that classified homosexual government employees as security risks, a position ratified by the Senate Subcommittee on Investigations. The liberal journalist Max Lerner devoted a twelve-part series to debunking myths about gay and lesbian mental health and proclivities but to little effect. More government employees were dismissed for sex deviation than for Communist sympathies.[^66]

[^66]:	Marcia M. Gallo, _Different Daughters: A History of the Daughters of Bilitis and the Rise of the Lesbian Rights Movement_ (New York: Carroll & Graf Publishers, 2006), 28; Kaiser, _The Gay Metropolis: 1940-1996_, 68-72.

The war also accelerated the medicalization of same-sex attraction and gender transgression. As psychiatrists examined draftees for their suitability for service, they derived criteria that would classify the population as normal or abnormal. Freud’s own 1937 judgment that homosexuality, while no advantage, was nothing to “be ashamed of, no vice, no degradation, it cannot be classified as an illness,” made no impression in a psychoanalytic atmosphere devoted to adjustment and orderly development. Instead, a sizable portion of the psychiatric and psychoanalytic communities devoted itself to “curing” homosexuality. Prevailing theory held same-sex desire as a result of incomplete Oedipal resolutions often caused by improper parenting. Few “cures” succeeded.[^67]


[^67]:	Allan Berube, _Coming Out Under Fire: The History of Gay Men and Women in World War Two_ (New York: Free Press, 1990).

Despite public condemnation, gay men and women often managed to live lives outside the closet in the immediate postwar years. In some fields, homosexual men and women lived in a liminal state --- not openly identifying as gay but not in denial either. Remarkably, a good number of gays and lesbians affirmed their belonging. In some fields, for instance, the homosexuality did them little damage. This liminal state in which one’s homosexuality was known to one’s peers but not publicly acknowledged was typical of the music and theater worlds. Musicians knew of Leonard Bernstein’s and Aaron Copeland’s homosexuality, which did not damage their careers as most likely would have been the case had they other professions. Bernstein went on to write the music for the incomparable _West Side Story_, working with other gay men, choreographer Jerome Robbins, lyricist Stephen Sondheim, and author Arthur Laurents. Robbins, a one-time Communist Party member, had resisted naming names until threatened with public revelation of his homosexual trysts.[^68] Civil rights activist and pacifist Bayard Rustin was not purged from organizations after his arrest for “lewd behavior,” although he was often shunted to the background.

[^68]:	Kaiser, _The Gay Metropolis_, 74.

The push for belonging intensified with the founding of two homophile societies in the 1950s. The Mattachine Society, founded in 1950 in Los Angeles by Henry Hay, a member of the Communist Party, and supported by designer Rudy Gernreich, originally aimed at establishing unity among gay men to fight anti-gay discrimination and police entrapment and from that struggle to create a “an ethical homosexual culture” modeled on that of “Negro, Mexican, and Jewish peoples.” The Daughters of Bilitis, an organization for lesbians begun in San Francisco by Phyllis Lyon and Del Martin in 1955, very soon defined itself as “A Woman's Organization for the purpose of Promoting the Integration of the Homosexual into Society.”[^69] Membership of both organization was at first small (Mattachine was patterned on the Communist Party with cells, different levels of membership, and a commitment to developing a group consciousness as an oppressed “class” in order to liberate themselves), but reached 2,000 in Southern California by 1953. Cold War worries led to the resignation of Hay and other Communists the same year.

[^69]:	Jonathan Katz, _Gay American History_ (New York: Crowell, 1976), 426.

The new leadership was considerably less militant. The legal director of the Mattachine Society accepted that the police were right to prevent public indecencies but hoped that authorities would accept that sexual activity between consenting adults was a private matter when done in private and should be of no interest to the law. Engaged in projects of public education, members of the homophile organizations rarely broached the notion of Fourteenth Amendment equal protection rights for all gays, emphasizing instead the need for gay respectability. The Daughters of Bilitis journal, _The Ladder_, printed on the inside cover the organization’s statement of purpose that emphasized education for the “variant” (one of the various euphemisms for homosexuals) in order that she would understand herself and hasten adjustment to society. To that end, the organization pledged to encourage dress and manners acceptable to mainstream society on the grounds that gender nonconforming behavior alienated potential allies. Historian Nan Boyd has characterized the attitude as “change can only be accomplished in the proper way and manner and by the proper people” and has noted that it did not prevent ordinary gay and lesbians to claim their own space and construct throughout the 1950s a burgeoning queer culture.[^70]

[^70]:	Boyd, _Wide-Open Town_, 173.

A few scientists provided intellectual support for the contention of homophile organizations that homosexuals shared attitudes and values with those in the mainstream. Alfred Kinsey had attracted attention, albeit often quite negative, for his placing sexuality on a spectrum rather than arguing for a hetero-homosexual divide. He had come to the position because his findings indicated that same-sex contact was more prevalent than expected. Less publicized but more significant was the work of Evelyn Hooker. A psychologist, Hooker undertook the study of homosexual mental health after the challenge of one of her homosexual students at UCLA. Based on her exchanges with him and his friends, she came to doubt the prevailing understanding of homosexuality as an illness. Aided by the Mattachine Society, she chose thirty homosexuals and thirty heterosexual men and paired them according to IQ and education. None had sought psychological help or spent time in prison. She administered her subjects the leading personality tests of the day, including the Thematic Apperception Test and the Rorschach Inkblot tests, and had the leading practitioners evaluate the tests without informing them of the sexual orientation of the test takers. Despite prior claims that the tests would reveal the inner and deformed internal makeup of gay men, the evaluators could discern no difference between homo- and heterosexual men, bolstering the case for complete inclusion. Although Hooker’s work first appeared in 1957 and was often duplicated, it took until 1973 for the American Psychiatric Association to remove homosexuality from its manual of disorders.[^71]

[^71]:	Henry L. Minton, _Departing From Deviance: A History of Homosexual Rights and Emancipatory Science in America_ (Chicago: University of Chicago Press, 2002), 219-36.

Perhaps the most fascinating account of the experience of homosexuality was Donald Webster Cory’s _The Homosexual in America: A Subjective Approach_ (1951). Cory was the pen name of Edward Sagarin and his book was part testimonial of his own wrenching struggles with his sexuality, part investigation, and part political statement. Cory set out to delineate both what it was like to be a homosexual and what social and political dynamics governed it. He disputed that homosexuality was unnatural since it had always existed, that it threatened the ability of civilization to reproduce itself, and that homosexuals were moral outlaws. The “sordid character of” gay men stemmed from the social attitudes that victimized them, he countered. “A person cannot live in an atmosphere of universal rejection, of widespread pretense, of a society that outlaws and banishes his activities and his desires, of a social world that jokes and sneers at every turn, without a fundamental influence on his personality.”[^72] Just as white society created the Negro problem, so too did heterosexual society create the homosexual problem. To be gay was to be caught in a vicious circle, having inequality forced on one but then having the consequences of that inequality used to justify inequality in the first place. The solution, Cory held, was to speak forthrightly as gay people. That gay men could so easily pass was not a blessing. “Actually, the inherent tragedy --- not the saving grace --- of homosexuality is found in the ease of concealment. If the homosexual were as readily recognizable as . . . other minority groups, the social condemnation could not possibly exist. Stereotype thinking on the part of the majority would . . . collapse of its own absurdity if all of us who are gay were known for what we are. . . . If only all of the inverts, the millions in all lands, could simultaneously rise up in our full strength!”[^73] A gay organization should aim not for social tolerance --- which Sagarin defined as the condescension of the superior for the inferior or the evil --- but for acceptance on equal footing.

[^72]:	Donald Webster Cory, _The Homosexual in America: A Subjective Approach_ (New York: Greenberg Publisher, 19151) 12.

[^73]:	Cory, 230.

Although Cory’s book inspired numerous gay and lesbian readers, he himself stayed in the closet for much of his life. As Sagarin, he went on to be a professor of sociology who specialized in deviance and contended homosexuality was not normal. Even as Corey in the late 1950s and early 1960s, he rejected Hooker’s dismissal of the deviance model as unscientific. Having undertaken his 1951 book to demonstrate that same-sex desire was “involuntary, as if inborn,” Corey accepted that homosexuality was attributable to family dysfunction and that the well-adjusted homosexual was a fiction. Efforts to win office in the New York Mattachine Society came to naught. Sagarin later opposed gay liberation and condemned the gender transgressions. Nonetheless Sagarin paved the way for numerous attacks on the closet. Jenette Howard Foster, who had worked as a librarian at Indiana University for Alfred Kinsey’s sex research projects, self-published _Sex Variant Women in Literature_ in the 1956. The book enjoyed something of a _samizdat_ life until reissued in 1975. By the late 1960s the gender non-conforming behavior and dress that appeared in numerous gay and lesbian cultures not only in large coastal cities of Los Angeles, San Francisco, and New York, but also, as Elizabeth Lapovsky Kennedy and Madeline D. Davis have demonstrated in the working-class precincts of Buffalo were commonplaces.[^74] That Sagarin/Corey did not recognized the realization of his wish that homosexuals speak as themselves is testimony how the struggle for inclusion created divisions not only among homosexuals but, in this case, in the personality of one of its original advocates.

[^74]:	Elizabeth Lapovsky Kennedy and Madeline D. Davis, _Boots of Leather, Slippers of Gold: The History of a Lesbian Community_ (New York: Routledge, 2014); Kaiser, _The Gay Metropolis_, 125-30.



### The Tentative Border Crossing of American Music


It was no coincidence that Norman Mailer’s “White Negro” depended so much on his attribution of anarchical impulses to jazz musicianship. For much of American history, whites have turned to black music as a source of energy, often to salve real and imaginary wounds. Such cultural appropriation has yanked the music out of its context, stereotyped black thought and culture as primitive and unschooled, and denied black musicians credit and remuneration. In the first half of the twentieth century, formal critical opinion dismissed the aesthetic quality of black and Latino musics as lacking complexity, rigor, or spirituality. Music industry executives were less devoted to hierarchies of quality than to a classification scheme that conformed to actually existing tastes. Such schemes took the uncertainty out of selling records and usually conformed to the sociological characteristics of the audience rather than any particular musical qualities per se. The pop, country, race (later rhythm and blues) charts designated music marketed to white middle class, white Southern, and black listeners respectively rather than a single style or form of musical expression. Race records, for instance, were a musical hodgepodge, lumping together gospel, electric and country blues, black pop, and doo-wop. Because they were rooted in race and class, the charts were assumed to define self-contained, mutually exclusive tastes.[^75] For many critics and record executives, postwar jazz constituted a different category, especially since many musicians during and after the war aspired to make music that resisted racial and commercial appropriation.

[^75]:	David Hajdu, _Love for Sale: Pop Music in America_ (New York: Farrar, Straus and Giroux, 2016); David Brackett, _The Pop, Rock, and Soul Reader: Histories and Debates_ (New York: Oxford University Press, 2005); Larry Starr and Christopher Alan Waterman, _American Popular Music From Minstrelsy to Mp3_ (New York: Oxford University Press, 2010).

At first glance, then, it seemed unlikely that white audiences would truly appreciate the skill, dedication, and intelligence of minority musicians or that white musicians would borrow black or Latin elements without parodying, exaggerating, or misconceiving them. Yet there were moments in the 1950s that constituted meaningful border crossings that approximated Ellison’s hope that whites would seriously appreciate the meaningful contributions of black culture. The possibility of new music cultures was opened with the breakdown of the rigid categories of the music marketplace. If charts, jukeboxes, and radio formats were still segregated, not all listeners’ tastes were. The airwaves, especially clear channels at night, were available to anyone with a set and the number of boundary crossings were significant. White consumers, especially teenagers, showed a significant interest beyond their parents’ taste. One such listener, Elvis Presley, demonstrated his love for rhythm and blues when he started playing Arthur “Big Boy” Crudup’s “That’s All Right Mama” during a break in what had been an unsuccessful recording session Sam Phillip’s Sun studio in Memphis. Border crossing was not solely white interest in black music. Black listeners, contrary to expectations, knew their country and western music. One such listener, Chuck Berry, incorporated the fiddle tune “Ida Red” into his first hit record “Maybellene” in 1955. It was the melding of influences, supported by crossover listeners that led to one of the crucial innovations of American music, rock ’n’ roll.



Miles Davis’s border crossing was even more eclectic, a demonstration of the degree to which jazz had become incessantly creative. A student at Juilliard, Davis simultaneously thought it too mired in European classical and white music and valuable for the music theory he learned. After dropping out, he went to work for bebop pioneer Charlie Parker. Parker’s great innovation was his playing the alto saxophone in flurries of sixteenth notes rather than the four steady beats per bar, piling new chords and syncopated melodies on top of extended chords. In addition, Parker became an idol to musicians such as Davis because Parker spurned the role of the musician as entertainer who pleased and courted his audience. When Davis struck out on his own in the late 1940s, he extended the stance becoming the epitome of the removed, fiercely proud black artist. He retained as well his predilection for middle range, muted tones. Working with Canadian-born arranger Gil Evans, Davis slowed down the furious pace of bebop and supplemented it with harmonies drawn from European impressionist composers. Noted jazz critic Gary Giddins pointed out that the combination of swing, bop, and classical techniques that produced “cloudlike chords in which the harmonies slipped seamlessly one to the next and breathlessly long phrases.”[^76] With _Birth of the Cool_ (recorded 1950, released 1957), Davis and Evans constructed a sound that used paired instruments in the nine-piece group to achieve what was termed a unison sound that Davis insisted sounded like human voices singing. By 1959, Davis had shifted again, incorporating pianist George Russell’s concept of tones that rested on a new mode, the Lydian, which emphasized fifths. Russell expanded upon early-modern church music to establish chromatic scales that had twelve tones. Working with pianist Bill Evans (no relation to Gil) and saxophonist John Coltrane, Davis expanded beyond bebop to multiply the available sounds. In _Kind of Blue_ (1959), Evans departed from usual practice when he did not play the full chords that players used to set the harmonies. Using these modes, the album, as the name suggests, constituted as much a meditation on the blues as a blues itself.[^77]

[^76]:	Gary Giddins, _Visions of Jazz: The First Century_ (New York: Oxford University Press, 1998), 340.

[^77]:	Ashley Kahn, _Kind of Blue: The Making of the Miles Davis Masterpiece_(Da Capo Press, 2000); Frank Alkyer, _The Miles Davis Reader: Updated Edition (the Downbeat Hall of Fame Series)_ (New York: Backbeat Books, 2018); Gerald Early, _Miles Davis and American Culture (Missouri Historical Society Press)_ (St. Louis: Missouri History Museum Press, 2001).

Not all interchanges were as creative or as respectful. To be sure, there was something amounting to a moral panic in the press from mental health professionals and religious authorities about the animal spirits that popular black music awakened in listeners. _Time_ Magazine claimed that fans of rock ’n’ roll were as mindless as Hitler’s followers.[^78] Such sentiments proved less authoritative than those who made them hoped. More lasting was the power of white entrepreneurs who negotiated particularly disadvantageous contracts with black performers (the disc jockey and impresario Alan Freed got songwriting credit for Chuck Berry’s “Maybellene” in return for pushing it to his considerable following). Black artists often found themselves elbowed out of mainstream attention when record executives employed white artists to do songs that black musicians had written, altering the lyrics, rhythm, or presentation to reduce any perceived threat from black male swagger and the defiance that went with it. Pat Boone, a Columbia University English major, was noteworthy for covers of Fats Domino and Little Richard hits, interspersed with crooning ballads. Boone’s versions sold exceedingly better among white teenagers, as much because his records were more often played and more often found in record stores that white record buyers frequented as because the majority only wanted versions that they regarded as more accessible. By the late 1950s and early 1960s, white musicians had mastered their imitations, bequeathing white music with in the words of Eric Lott “the cool, virility, humility, or _gaite de couer_ that were the prime components of white ideologies of black manhood.”[^79]

[^78]:	Glenn C. Altschuler, _All Shook Up: How Rock ‘N’ Roll Changed America_ (New YorkOxford University Press, USA, 2004), 6.

[^79]:	Eric Lott, _Love and Theft: Blackface Minstrelsy and the American Working Class_ (New York: Oxford University Press, 1993), 32.

The cultural interaction involved in the song “Hound Dog” entailed both collaboration and exploitation, ventriloquism and fruitful exchange. It is the story of a song written by two Jewish songwriters for a gender transgressive female blues singer and redone in a new fashion by a white working-class Southerner. The songwriters were Jerry Leiber and Mike Stoller, who fell in love with black culture and produced hits for such black groups as the Coasters and the Drifters. By most accounts, group members were impressed with the depth of black cultural knowledge that the pair had. For all their aspiration to overcome long-standing notions of black inferiority and commitment to cultural unity, Leiber and Stoller nonetheless emphasized black clownishness and black sexuality in ways they could not fully control. In Big Mama Thornton, they wrote for a singer whose short hair, male clothing, and gruff manner made her a distinctive female presence. They expected she would growl the song that drew upon the blues motifs to tell the story of a woman who rids herself of a gigolo but Thornton’s first run-through was more akin to crooning. Efforts to tell her how to sing led to her counter that a white boy had no standing to tell her how to sing the blues that she grew up with. Eventually a rapprochement was reached and Thornton added her own interpretative elements, which led her at one point to claim ownership of the song in an effort to get fair remuneration from Houston-based Peacock Records. “Hound Dog” rose to the top of the r&b chart in 1953 and would have sold even more copies had other labels not issued their own versions and “answer” records.[^80]

[^80]:	See John Covach, _What’s That Sound: An Introduction to Rock and Its History_ (New York: W. W. Norton, 2009); Michael T. Bertrand, _Race, Rock, and Elvis_ (Urbana: University of Illinois Press, 2000); Peter Guralnick, _Last Train to Memphis: The Rise of Elvis Presley_ (Boston: Little, Brown, and Co., 1994).

Although he knew the original, it was a parody version that Elvis Presley really enjoyed when he saw it performed in Las Vegas some three years later. Presley’s version, which featured machine-gun drumming and an unusual guitar solo, lacked Thornton’s double entendres and her blues rhythm. In its stead, Presley added a Latin riff and a sneer (supposedly anger at being made fun of during a television appearance). A testimony to eclectic tastes, Elvis’s 1956 version topped the r&b and country charts and reached number two on the pop chart. _Melody Maker_ dismissed the record as a thoroughly bad record, loud and incoherent. Bob Dylan, on the hand, claimed that hearing it in Minnesota at the age of fifteen changed his life. For their part, Leiber and Stoller did not like Presley’s version, thinking it too nervous, too fast, and lacking Thornton’s bite. Presley thought it a diversion, the silliest song he ever recorded.

Elvis had a less conscious relationship with other cultures than Leiber and Stoller. He grew up close to black neighborhoods in Memphis and bought his clothes at the leading clothier for black people. He, of course, deeply imbibed black blues and rhythm-and-blues (but not jazz, which never found favor with poor whites). Treating it as part of his environment, he admitted to _Jet_ magazine that “nobody can sing that kind of music like colored people . . . I know that. But I always liked that kind of music.”[^81] The case for Presley being part of the white minstrelsy tradition of love and theft is far from open and shut. Being part of the southern white working class, he did not feel the stultifying nature of all-white suburban life that prompted a turn of black culture for energy and exoticism. Although the white working class had the privilege of white skin, it experienced a measure of denigration and dismissal. Presley drew on white-trash traditions of masculinity as well as black ones. For his Sun Records sides, he melded a blues with a country beat. This combination of various elements, when played faster than the blues, was extremely danceable. Originally rejected by country stations as too black and rhythm and blues stations as too white, Elvis achieved musically what previous rockabilly musicians who had mixed country and rhythm and blues had not. Where they had seasoned their country elements with rhythm and blues, Elvis more thoroughly merged them, producing a music that Peter Guralnick terms “nervously up tempo.”[^82]

[^81]:	Quoted in Bertrand, _Race, Rock, and Elvis_, 199.

[^82]:	Peter Guralnick, “Rockabilly,” in Jim Miller, ed. _The Rolling Stone Illustrated History of Rock & Roll_ (New York: Rolling Stone Press, 1976), 64-67.

In the end, it was less his sound than his sexuality that attracted the most attention---exciting audiences and offending censorious critics with his gyrations. His self-presentation earned him the derisive name “Elvis the Pelvis” and prompted Ed Sullivan famously to shoot him from the waist up during his performances. Black rock ’n’ roll pioneers who hoped to cross over with white audiences had to navigate the pitfalls inherent in sexual expression more deliberately. For Little Richard, born Richard Penniman, removing the threat of black male sexuality was through outright exaggeration to nearly comic proportions. Sporting a flamboyant homosexuality and wild man persona, he combined the outlandish stage presence with lyrics such as the original ones for Tutti Frutti (1956)--“Tutti frutti, loose booty If it don’t fit, don’t force it You can grease it, make it easy”--and the slightly less scandalous “Good golly, Miss Molly, you sure like to ball.” Both his manner and his lyrics made him ripe for covers by white artists. Chuck Berry took the opposite tack. He attempted to defuse tensions in his witty, well-crafted lyrics that, unlike those of rhythm and blues, obscured or removed his physical presence from the songs. Eschewing bragging, he wrote in the third person or quickly turned attention away from his feelings and desires, as he did in “Maybellene” by making the car chase paramount (even if it could be interpreted as a double entendre). Much of his early success came from his ability to pitch his music to a broader integrated audience with songs that spoke more directly to the high school and teenage experience (“Oh Baby Doll” and “School Days”). The persona failed to protect him, however. Charged with violating the Mann Act for having relations with a 14-year old he brought from Mexico to be a hatcheck girl in his St. Louis nightclub, Berry was convicted by an all-white, all-male jury in a trial that had to be retried because of the racist comments of the presiding judge.

Penniman and Berry did bequeath to popular music the elements that would be its building blocks for the 1960s and beyond---the backbeat you can not lose, the volume, and the tempo that insured the prominence of the beat. Berry was particularly innovative. Like Little Richard he merged boogie-woogie played at eight-beats to a bar, which changed the shuffle that boogie-woogie usually had, with a backbeat, which emphasis on even rather than initial beats. The first “guitar hero,” Berry often played double notes on the high strings, which gave the music an unprecedented density. Throughout the career that followed Berry was alternatively celebrated (Beatle John Lennon once said rock ‘n’ roll should be called “Chuck Berry”) and copied relentlessly. At the height of his career, however, he was little recognized for his contributions. That lack of recognition pointed to the real difficulties of crossing aesthetic and social borders on equal terms in the postwar period. Berry himself made few references to racial matters in his work---a line in “Handsome, Brown-Eyed Man” refers to being arrested on charges of unemployment---but perhaps one could see a larger hope for border-crossing culture in a verse from “School Days” in which the singer escapes from the drudgery of school for the joys of playing rock ‘n’ roll jukebox, which will “deliver me from days of old.” It remains hope that has yet to be realized.
